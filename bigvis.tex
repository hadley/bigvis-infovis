\documentclass[journal]{vgtc}                % final (journal style)
%\documentclass[review,journal]{vgtc}         % review (journal style)
%\documentclass[widereview]{vgtc}             % wide-spaced review
%\documentclass[preprint,journal]{vgtc}       % preprint (journal style)
%\documentclass[electronic,journal]{vgtc}     % electronic version, journal

%% Please note that the use of figures other than the optional teaser is not permitted on the first page
%% of the journal version.  Figures should begin on the second page and be
%% in CMYK or Grey scale format, otherwise, colour shifting may occur
%% during the printing process.  Papers submitted with figures other than the optional teaser on the
%% first page will be refused.

\usepackage{mathptmx}
\usepackage{graphicx}
\graphicspath{{./images/}}
\usepackage{times}
\usepackage{natbib}
\usepackage{booktabs}
\usepackage{amsmath}

%% This turns references into clickable hyperlinks.
\usepackage[bookmarks,backref=false,linkcolor=black]{hyperref} %,colorlinks
\hypersetup{
  pdfauthor = {},
  pdftitle = {},
  pdfsubject = {},
  pdfkeywords = {},
  colorlinks=true,
  linkcolor= black,
  citecolor= black,
  pageanchor=true,
  urlcolor = black,
  plainpages = false,
  linktocpage
}

%% If you are submitting a paper to a conference for review with a double
%% blind reviewing process, please replace the value ``0'' below with your
%% OnlineID. Otherwise, you may safely leave it at ``0''.
\onlineid{0}

%% declare the category of your paper, only shown in review mode
\vgtccategory{Research}

%% allow for this line if you want the electronic option to work properly
% \vgtcinsertpkg

%% In preprint mode you may define your own headline.
%\preprinttext{To appear in an IEEE VGTC sponsored conference.}

%% Paper title.

\title{Bin-summarise-smooth: A framework for visualising large data}

%% indicate IEEE Member or Student Member in form indicated below
\author{Hadley Wickham}
\authorfooter{
%% insert punctuation at end of each item
\item
 Hadley Wickham is Chief Scientist at RStudio. E-mail: h.wickham@gmail.com.
}

%other entries to be set up for journal
\shortauthortitle{Wickham: A visualisation framework for in-memory big data}

%% Abstract section.
\abstract{

Visualising ``big'' data is challenging both perceptually and computationally: it is hard to know what to display and hard to efficiently display once you know. To tackle both problems, this paper outlines a framework for displaying large in-memory datasets (i.e.\ on the order of 100 million observations) on commodity hardware. It is based around a four step process: group, summarise, smooth, and visualise. Binning and summarising efficiently (O(n)) condense the large original data into a size suitable for display (recognising that there are on order of 3 million pixels on a screen). Smoothing helps resolve statistical problems from binning and summarising, and because the data is smaller, can use more sophisticated algorithms.

We provide a single-core in-memory implementation with the {\tt bigvis} R package that produces static visualisation, which uses C++ for internal high-performance implementation, and an R wrapper to provide a user-friendly interface for analysts. The framework is readily extensible to parallel out-of-memory solutions, and to interactive and dynamic graphics.
} 

%% Keywords that describe your work. Will show as 'Index Terms' in journal
%% please capitalize first letter and insert punctuation after last keyword
\keywords{Big data, kernel smoothing}

%% ACM Computing Classification System (CCS). 
%% See <http://www.acm.org/class/1998/> for details.
\CCScatlist{ % not used in journal version
  \category{H.5.2}{Information Interfaces and Presentation}%
  {User Interfaces --- Graphical user interfaces (GUI), Interaction styles, Screen design, Evaluation/methodology}
  \CCScat{I.6.8}{Computing Methodologies}%
  {Simulation and Modeling}{Visual Simulation};
}

%% Uncomment below to include a teaser figure.
\teaser{
  \centering
  \includegraphics[width=5.33cm]{teaser-1}%
  \includegraphics[width=5.33cm]{teaser-2}%
  \includegraphics[width=5.33cm]{teaser-3}
  \caption{Refining visualisation of a 76 flight times exploring average delay (colour) as a funtion of distance (x axis) and speed (y axis). (Left) All data, (center) the middle 99.5\% of the data, (right) average delayed transformed to shrink the tails (a modulus trasformation with parameter 0.25). Unsuprisingly, flights with higher than average speeds for their distance (top-right) have shorter delays (red); more interestingly there's a set of shorter slower flights (bottom-left) that have average delays close to 0 (white).}
}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%% START OF THE PAPER %%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}
\firstsection{Introduction}

\maketitle

As data grows ever larger, it is important that our ability to visualise it grows too. This paper presents a novel framework for visualising large data that is designed to be both computationally and statistically efficient, dealing with both the challenges of what to display for very large datasets and how to display it fast enough for interactive exploration.

The insight that underlies this work is simple: the bottleneck for visualising large datasets is the number of pixels on a screen. For 1d dimensional summaries, we need on the order of 3,000 pixels; for 2d summaries, we need 3,000,000 pixels. There is no point displaying more data than that, and rathering than relying on the graphics rendering engine subsystem to reduce the data, we develop summaries built on well-known statistical principles. Interaction adds another dimension (time). While interaction is not the main focus of this paper, I will show how the framework does support interactive environments.

The framework involves four steps: binning, summarising, smoothing and visualising. This captures the most important 1d and 2d plots for statistical data: histograms and frequency polygons, kernel density estimates, local regression, loess \citep{cleveland:1979}, boxplots, and smoothed quantile regression \citep{koenker:2005}. It is also readly extensible to new visualisations that instead focus on higher moments (like standard deviation, skewness and kurtosis) instead of the mean or median.

After a review of existing work, Section~\ref{sec:related-work}, much of which occured in the early 90s as statisticians worked to transition their algorithms from mainframes to PCs, the paper explores each component in turn.


Section \ref{sec:bin} discusses binning. Fixed width binning is simple, but it's good to understand the drawbacks.

Picking good summaries involves a marriage of computational and statistical  concerns. Computationally, we want linear behaviour and the ability to parallelise, and statistically, we want summaries that are resistant to unusual values. Section~\ref{sec:summarise} discusses these tensions. The computation of binning and summary reduces the data from size $n$ to size $m$, where $m$ is typically 3,000 for 1d summaries and 3,000,000 for 2d summaries.

Tension between a flexible system that doesn't restrict the analyst to preconcieved visualisations, but equally has to be constrained to a domain where we can compute efficiently. The system based on binning and smoothing balances this tension well: binning is an extremely simple operation that can be implemented very efficiently; it has deficiencies but these can be compensated for with a smoothing algorithm. Smoothing is statistically efficient, but computationally hard, but smoothing binned data is fast and looses little statistical strength. The statistical literature is particularly important since it helps justifies our approximations, showing that they do not result in untoward loss of information.

Smoothing, as discussed in Section~\ref{sec:smooth} is a critical part of this framework because it makes it possible to remedy some of the statistical problems with the simple, but fast, binning and summary steps. Because the smoothing occurs on the condensed data, the approach can be computationally much more expensive. This section summarises and unifies the existing extensive statistical work in this area.

Even once we've reduced the data to manageable size, visualisation of large data presents some special challenges. In Section~\ref{sec:visualise}, we discuss generally how to visualise the condensed datasets, and some of the particular problems with very large data, notably problems caused ``unusual'' values.  Focus on the most important 1d and 2d statistical graphics: the histogram and the scatterplot.

Section~\ref{sec:bigvis} discusses the open-source reference implementation that accompanies this paper. The R \citep{R} bigvis package, available from \url{http://github.com/hadley/bigvis}, uses a C++ for high-performance component and R for a user friendly CLI front end.  Additionally, the code to reproduce this paper and accompanying figures can be found at \url{http://github.com/hadley/bigvis}.

The reference implementation provides some of the constraints that guide this paper. It is designed to be used by experienced analysts who will be using  sophisticated statistical and machine learning models in conjunction with visualisation.  It allows the analyst to create plots of 100,000,000 observations in under 5s. $10^8$ doubles occupy slighty under 800 Mb of memory, so about 20 vectors can be stored in 16 Gb of ram, with about 1 Gb left over. Support tiered model of data access: analyst may have access to billions or trillion of observation in a multi-machine data warehouse, but for most problems some interactive exploration is needed on a small subset or aggregate. 5s is an interactive timeframe for a REPL.  This is arbitrary but is currently about how long it takes to draw a scatterplot of 200,000. It's well above the threshold for direct manipulative interaction, but my personal experience seems to be on the order of what I expect when doing interactive data analysis in R.  Too much longer and I start chcking my email and my productivity goes down hill.

Section~\ref{sec:conclusion} concludes with directions for future work, discussing how this framework extends naturally to multi-core, out-of-memory data, and how it could be adapted for the interactive analysis of large data.

\section{Related work}
\label{sec:related-work}

% Related statistics work
There is an extensive body of statistical research on density estimation (smooth histograms) and smoothing, stretching back to the early 70s. Three good summaries of the work are \cite{scott:1992,bowman:1997,loader:1999a}.    \cite{hardle:1992,wand:1994,fan:1994} focus on the computational side of the smooth estimators, and provide good advice for efficient implementation.  Surprisingly none recognise that simple kernel smoothing is a convolution, and in high-dimensions rearranging the order of the convolution can change the problem from $O(n^d)$ to $O(nd)$. But these papers only treat special cases of this framework, fail to treat some of the problems associated with visualising large data and are not accompanied by code.

% Infovis and kde
\citep{lampe:2011} uses kernel density estimates in an infovis context, but fails to connect their work to existing statistical work. Provide a very fast implementation on the GPU, which presumably could be made even faster if they read previous work. Other techniques from infovis are related to kernel density estimation. Footprint splatting \citep{becker:1997,yang:2003} is exactly equivalent to kernel density estimation. Similarly, using transparency as a way of dealing with overplotting, as in ..., can be seen as a special type of kernel density estimate where the kernel is a uniform radial point.

% Other papers on large data
There are other techniques to deal scatterplots of large data. Hexbin plots \citep{carr:1987} bin 2d data using hexagons to avoid the perceptual issues related to gridding. Generalised scatterplots \citep{keim:2010} use distortion, but no discussion of computational performance. \citep{heer:2012} discusses the challenges of visualising large data, and proposes interaction as a solution. Emphasises the challenge of data cleaning and manipulation.

\section{Bin}
\label{sec:bin}

The first stage of the process is to condense the data, reducing millions of data points to approximately one summary per pixel available on the screen.  To do this we must first determine how to group the data into bins; in other words, we need to assign each data point to an integer. We want an injective mapping, so that many data points will be mapped to a single integer; this is the chief means by which this framework deals with large data. Important to think of this as a separate step, because it may often be performed outside of the visualisation environment, for example in the database layer. 

The most important type of binning is {\bf fixed width binning}. Fixed width binning is parameterised by two variables, the origin and the width, and can be computed extremely efficiently using $\left \lfloor (x - origin) / width \right \rfloor $. Fixed width binning is extremely computationally efficient, easily extended from 1d to nd, and while statistically na\"ive, there is little evidence that variable binwidths do better.

1d binning is easily extensible to multiple dimensions. We first bin each dimension, giving a vector of integers, $(x_1, x_2, ..., x_m)$. It is simple to devise a bijective map between a vector of integers and a single integer if we know the highest bin in each dimension. If we have $m$ dimensions, each taking possible values $0, 1, \ldots, n_m$, then we can collapse the vector of integers into a single integer using this formula:

\[ x_1 + x_2 \cdot n_1 + x_3 \cdot n_1 \cdot n_2 + \cdots + x_m * \Pi^{n-1}_{i = 1} n_i = \]
\[ x_1 + n_1 \cdot (x_2 + n_2 \cdot (x_3 + \cdots(x_m)) \]

It is easy to see how this works if each dimension has ten bins. For example, to project 3d bin $(5, 0, 4)$ into 1d, we compute $5 + 0 * 10 + 4 * 100 = 4 + 10 * (0 + 10 * 5) = 405$. Given a single integer, we can find the nd bins by reversing the transformation, peeling off the integer remainder after dividing by 10. For example, 1d bin $356$ corresponds to 3d bin $(6, 5, 3)$.

This function turns out to be a monotone minimal perfect hash \citep{belazzougui:2009}, which means highly efficient data structures are available to work with it. (A perfect hash eliminates a equality comparison). Even if this data structure is not used (as in the reference implementation), it easy to efficiently summarise bins in high-dimensions using standard data structures (either a vector if most bins have data in them, or a hash if not).

Statistically, there is a small issue with fixed bins: they tend to be too variable in regions of few data points (because the error of summaries is typically proportional to $\sqrt(1/n)$). Ideally, we would vary the width of the bins so that there are more bins where we have more data and fewer bins where we have less data. However evidence from asymptotic analysis is not promising: for the histogram, \cite{kogure:1987} found that fixed bins were globally optimal across all possible variable size algorithms under fairly wide assumptions. The challenge is that as we expand the space of possible binwidths, finding the optima in that space becomes significantly harder. 

% Linear binning vs. simple binning

% proviso that the bin is a integer, so can not store more than $2^32$ possible bins. (This seems like a lot but shrinks quickly as the number of dimensions grows: it's 85 bin in each direction in 5d and only 9 bins in each direction in 10d).  Outside that range can switch to standard hashing algorithms, albeit with a performance penalty because the algorithm will be more complex, and additional equality comparisons will be required. (Read citations on \url{http://en.wikipedia.org/wiki/Perfect_hash_function} and cite appropritately)

\section{Summarise}
\label{sec:summarise}

Once each of the $n$ original data points has been placed into an integer bin $m$ ($m \ll n$), the next step is to collapse all of the $n_i$ points in each bin into a fixed number of summary statistics, like the count, mean or standard deviation. Summary statistics involve a tension between computational efficiency, robustness and accuracy.

A back-of-the-envelope calculation gives us a performance target: assuming we want to condense 100 million observations into 10,000 bins in under 5s, we need to be able to compute each summary of 10,000 observations in 0.5 ms, which implies we have no more than 50 ns to process each observation. This is achievable with a high-performance language like C++. In the reference implementation, summarising $10^8$ observations in $10^4$ bins takes $\sim 2.8$s for the mean, $\sim 3$s for the variance, and $\sim 5.8$s the median. \footnote{Timings performed on a 2.6 GHz Intel Core i7, 15\" MacBook retina} \footnote{These are likely to be upper bounds as I have only programmed in C++ for 6 months, and did not use any performance tricks}.  More details are provided in Section~\ref{sub:benchmarks}.

It is useful to consider possible summary statistics in view of the classification of Gray et al. \citep{gray:1997}. A summary is:

\begin{itemize}
  \item {\bf distributive} if it can be computed using a single element of interim storage, and summaries from subgroups can be combined. This includes count, sum, min, and max.
  
  \item {\bf algebraic} if it is a combination of a fixed number of distributive statistics. This includes the mean (count + sum), standard deviation (count + sum + sum of squares) and higher moments like skewness and kurtosis.
  
  \item {\bf holistic}, if it requires interim storage that grows with the input data. This includes quantiles (like the median), count of distinct elements or the most common value. 

\end{itemize}

Algebraic and distributive statistics are important because results from subgroups can easily be combined. This has two benefits: it makes parallelisation trivial, and it supports a tiered approach to exploration. For example, if you have 100 million observations, you might first finely bin into 100,000 bins. Then for any specific 1d plot, you rebin or subset, which is much faster than working with the original data. This tiered approach is particularly useful for interactive visualisations; the fine binning can be done upfront when the visualisation is created, then modifying the binwidth or the plot ranges can be done at interactive speeds.

The mean, standard deviation and higher moments can all be computed in a single pass, taking $O(n)$ time and $O(1)$ memory. Some care is needed as naive implementations (e.g.\ computing the variance as $\sum_i^n \frac{x_i^2}{n} - \left( \sum_i^n \frac{x_i}{n} \right)^2$) can suffer from severe numerical problems, but better algorithms are well-known \citep{welford:1962}. The median also takes $O(n)$ time (using the quick-select algorithm), but needs $O(n)$ memory: there is no way to compute the median without storing at least half of the data, and given the median of two subgroups, no way to compute the mean of the full dataset.

There is an interesting tension between the mean and the median: the median is much more robust to unusual values than the mean, but requires unbounded memory. A useful way to look at the robustness of a statistic is the {\bf breakdown point}. The breakdown point of a summary is the proportion of observations an attacker needs to control before they can arbitrarily influence the resulting summary. It is 0 for the mean: if you can influence one value, you can force the mean to be any value you like. The breakdown point for the median is 0.5: you have to taint 50\% of the observations before you can arbitrarily change the median. The mean is computationally desirable, but is less statistical desirable since just one arbitrary value can arbitrarily taint the summary.  This is a general tension: the easiest summary statistics to compute are also the least robust, while robust statistics are usually holistic.

It is often possible to convert a summary from holistic to algebraic by taking an approximation. For example, the median can be approximated by the remedian \citep{rousseeuw:1990}, the count of distinct values by the hyperloglog  algorithm \citep{flajolet:2007}, and other quantiles by a variety of methods \citep{finkelstein:1994,hurley:1995,liechty:2003}. Others have proposed general methods for adapting any holistic summary \citep{christmann:2007}. This is an active area of research, and is particularly important for streaming data as there is no way to hold all data in memory.  

% Robust alternatives to the standard devation often depend on pairwise distances \citep{rousseeuw:1993}, \citep{croux:1992} 

Even if you do use robust statistics, you are only protected from scattered outliers, not a radical mismatch between the data and the summary: a mean and sd will never be a good summary if the data has two strong modes. For this reason, visualisation must be iterative - you can not collapse a variable to a summary until you have some confidence that the summary isn't throwing away important information. In practice, practitioners may need to develop their own summary statistics for the pecularities of the data they use; the ones discussed here should provide a good start for general use.

Finally, there is no reason to limit ourselves to only 1d summary functions. 2d summaries like the correlation, or slope of a linear model may also be interesting. All statistics from a linear model can be computed in $O(n)$ in time and $O(1)$ in space \citep{miller:1992}, are and thus suggest a fruitful ground for generalisations to higher dimensions. Other 2d summaries that can be computed quickly will also be useful; scagnostics \citep{wilkinson:2005} are an obvious starting place.

\section{Smooth}
\label{sec:smooth}

Smoothing is an important additional step after summarising because it allows us to resolve problems with excessive variability in the summarised statistics. This variability may arise because the bin size is too small, or because there are unusual values in the bin. There are many approaches to smoothing, but we suggest a family of kernel based methods, because:

\begin{itemize}
  \item they are simple to implement and are fast.
  
  \item performing kernel smoothing on binned data instead of raw data adds insignificant error \cite{wand:1994} 
  
  \item kernel methods are approximately equivalent to other more complicated types of smoothing \citep{silverman:1984}.
  
  \item kernel methods form the heart of many existing statistical visualisations such as the kernel density estimator \citep{scott:1992} and average shifted histogram \citep{scott:1985} and loess \citep{cleveland:1979}.

\end{itemize}

Note that when applied to count summaries, these methods are subtly different from a kernel density estimate. A kernel mean is similar to a kernel density estimate: but a kernel mean divides by (normalises by) by the total weight at each point. This change is most noticeable with small bandwidths: the kernel density estimates will be very large (because they must integrate to 1), and outside the range of the data the estimates will be 0, where outside the range of the data the smoothed counts will be undefined (0/0).

There are three techniques in our family of smoothers: kernel means (aka Nadaraya-Watston smoothing), kernel regression (aka local regression) and robust kernel regression (aka loess). These effectively improve upon a simple moving average, and provide a smooth tradeoff of computational performance vs.\ statistical performance. While closely related, these methods developed in different parts of statistics at different times, and the terminology is inconsistent. \citep{cleveland:1996} provides a good historical overview of kernel smoothing methods.

The intution for these methods are simple, we want to smooth the value at each location by taking into account the neighbouring points. The weight closer points more heavily than distant points we use a {\bf kernel function}. Many kernels are available, but there is little evidence to suggest that the precise form of the kernel is important. Gaussian is common, but we use the triweight, $K(x) = (1 - |x^3|)^2 I_{|x| < 1}$, because it is bounded and simple (evaluation of this function is about 10x faster than evaluation of the Gaussian kernel).  

At each binned location we have $x_i$, the center of the bin, $y_i$ the summary of that bin and $w_i$ the number of observations in that bin. To predict a smooth estimate at position $j$, we first compute the kernel weights for each location $k_i = K(\frac{x_j - x_i}{h})$. The parameter $h$ is called the bandwidth, and controls the degree of smoothing: the larger $h$ is, the more neighbours are used to compute the smooth and the smoother the final curve. Note that because of the form of the triweight kernel, any observation more than $h$ away from $x_j$ will not contribute to the smoothed value.This allows for efficient computation.

The weighted mean is fastest to compute but suffers from bias on the boundaries, because the neighbours only lie on one side.  Also as \citep{macaulay:1931} puts it: ``A little thought or experimentation will quickly convince the reader that, so long as we restrict ourselves to positive weights, no moving average, weighted or unweighted, will exactly fit any mathematical curve except a straight line.''  The weighted regression overcomes these problems by effectively using a first-degree Taylor approximation.  Higher-order approximations can be used (by fitting higher order polynomials in the model), but there seems to be little additional benefit in practice \citep{cleveland:1996}.  Finally, the robust regression iteratively down-weights the effect of points far away from the curve, and reduces the impact of unusual points at the cost of increased computation time.

The three kernel smoothing techniques is readily extended to higher dimensions, using corresponding higher dimensions means and regressions. Optimisations are also available. Kernel weighted means are particularly easy to generalise because they are a convolution, and due to the associative nature of convolution it's possible to perform an nD smooth through a sequence of 1d kernel smooths. This is an important optimisation because it converts an $O(n^d)$ process to $O(nd)$.  You can treat local regression in a similar way, although it is only approximate if the grid of values is not uncorrelated. Approximate robust local regression does not work.

While the fast fourier transformation is a common technique to speed up discrete convolution, it is not effective here.  Using the FFT makes code considerably more complicated (extra padding is needed to avoid the periodic nature of the FFT), and less flexible. Direct convolution with nested loops also makes it easier to predict smoothed values at specified locations, which it useful for cross-validation.

\subsection{``Automatic'' bandwidth selection}

Kernel smoothing introduces a tuning parameter: the bandwidth. There is much research on how to pick the ``best'' bandwidth, but it depends critically on what best means and the specific type of smoothing. Typically, integrated mean squared error is used, which may not correspond with what we actually want for visualisation \citep{denby:2009}. We take a pragmatic attitude and use leave-one-out cross-validation (loocv) \citep{efron:1983} to provide a starting point for the user.  This is supported by \citep{loader:1999}.

The intution behind loocv is simple: we compare the actual statistic at a location with its smoothed estimate computed without that observation. We then summarise all errors using the root mean squared error (rmse): $\sqrt{ \sum (y_i - \hat{y}_i)^2 / n}$. We can explore this summary across a range of possible grid values, or use standard numerical optimisation (L-BFGS-B \citep{byrd:1995}). While this is a good starting point, and is optimal in the sense of predicting new values, in our experience, this procedure tends to pick a bandwidth that is too small, preserving features in the data that while may be important, distract from the overall picture.

With the bounded kernel we use, this is straightforward to implement efficiently, in $O(m b)$ where $b$ is the number of bins covered by the kernel ($b \ll m$). \citep{fan:1994} suggests incremental algorithms that may reduce computation time still further.

\subsection{Varying bandwidths}

Intuitively, it seems like we should be able to do better with an adaptive binwidth: using small bins where there is more data, or where the curve is especially wiggly. There are many approaches documented in the literature  \citep{terrell:1992, brockmann:1993,schucany:1995,herrmann:1997}. We follow the simple, but performant, approach outlined in \citep{fan:1995}: to find a per-location $h$, we divide the data into $m / (10 * \log_{10}(m))$ pieces, estimate the optimal bandwidth for each piece separately, then smooth the estimates.

% Example from Fan paper. 

\section{Visualise}
\label{sec:visualise}

Relatively straightforward to visualise the resulting data, but we discuss a number of important issues to consider both statistically and visually.  Figure~\ref{fig:vis-types} illustrates the basic types of graphics for 1d and 2d summaries. Visualising counts vs. visualising other summary statistics.  Important to distinguish because all other summaries must be read taking into account how much underlying data there is: you don't want to draw conclusions about areas that have relatively few data points.

Note that for a 1d summary of counts we use a frequency polygon \citep{scott:1985a}, where the centers of each bar are connected with a line. This unifies the display of counts and other summaries; makes it easier to overlay multiple distributions (if summarised by another categorical variable); and asymptotic arguments suggest that it has slightly lower error than a histogram.

\begin{table}
  \scriptsize
  \centering
  \begin{tabular}{lll}
  \toprule
  Dimension & Count & Other summary\\
  \midrule
  1d & Frequency polygon & Line plot \\
  2d & Heatmap, Contour plot, Hypsograph & Heatmap + alpha\\
  \bottomrule
  \end{tabular}
  \caption{Basic visualisation components}
  \label{fig:vis-types}
\end{table}

On top of these basic plots, we add three suggested design principles for visualisation of large datasets:

\begin{itemize}
  \item Preserve missing values
  \item Prepare for outliers
  \item Display uncertainty
\end{itemize}

The product plots \citep{me:prodplots} framework provides a way of generalising these plots to higher dimensions, and for integrating with other categorical variables. For count data, we can draw on insights from . Once continuous data has been binned, it can be treated the same way as discrete data and it falls naturally into the framework of product plots. A small additioanl constraint is that only the final level can been a continuous display; which implies that even if higher levels are continuous they are coarsely binned .  It is also interesting to think about the limit of plots as the number of bins goes to infinity.  The product plots framework shows how these ideas can be adapted to display arbitrary dimensions.

\subsection{Missing values}

Another design principle (inspired by that of R) is that missing values should never be omitted without user intervention: they should always be preserved along the entire route from raw data to final visualisation. It is possible to remove them, but it must be a deliberate decisions: you never want to inadvertently distort your visualisations by silently leaving off data.

This principle also needs to be carried through all other components of the framework - missing values must be correctly binned, summarised and smoothed.

\subsection{Missing values}

As the size of data grows, the probability of finding very unusual values also grows. In practice, we have found the visualisation of most large datasets look like Figure~\ref{fig:empty} - most of the data is concentrated in a relatively small error of the plot. We have two tools to deal with this: peeling, when the outliers are spatial; and the modulus transformation, when the outliers are in colour, size or another aesthetic.

\subsubsection{Peeling}

A simple approach to deal with spatial outliers is to remove the smallest outer values. We call this approach peeling, and have implemented it by progressively removing the smallest counts on the convex hull of the dataset.

Typically, you don't want to ignore these unusual values, but instead you partition your analysis to look at the most common and at the unusual. Some small amount of peeling (often $< 1\%$) seems to improve most 2d plots, drawing the focus to the bulk of the data.  For plots of non-count summaries, this also has the additional benefit of reducing outliers in the summary dimension. 

\subsubsection{Modulus transformation}

The modulus transformation \citep{john:1980} is a generalisation of the box-cox transformation \citep{box:1964} to deal with positive and negative data. It provides a flexible tool to shrink or expand the tails of a distribution.

\begin{equation}
\begin{cases} 
  \text{sgn}(x) \cdot \log(|x| + 1) & \text{if $\lambda = 0$,} \\
  \text{sgn}(x) \cdot \frac{(|x| + 1)^\lambda - 1}{\lambda} &\text{if $\lambda \ne 0$.}
\end{cases}
\end{equation}

This transformation is particularly useful in conjunction with interactivity, as it allows the use to dynamically focus on important parts of the distribution.

\subsection{Error}

Vital that these displays

\section{The {\tt bigvis} package}
\label{sec:bigvis}

A reference implementation of these ideas is provided in the .  The \url{http://github.com/hadley/bigvis} and the sources for the figures in this paper (including all data) can be found at a \url{http://github.com/hadley/bigvis-infovis}

Rcpp package which provides an easy way to access R's internal datastructures with a clean C++ API.  C++ is extremely efficient and the data structures and algorithms provided by the STL make programming considerably faster.  The C++ code is by no means expert and uses only a smattering of advanced C++ functions; I have been programming in C++ for less than six months. There are likely to be considerable opportunities for further optimisation.

For efficiency, the C++ makes extensive use of generic functions to avoid the cost of virtual method lookup (this is small, but it adds up when working with 100's of millions of observations).  This makes connecting R and C++ somewhat challenging since generic functions are specialised at compile time. To work around this problem, a small code generater generates all specialised versions of template functions using a naming convention, and then in R code the specialised function name is generated dynamically.  This is somewhat inelegant, but unavoidable when coupling programming languages with such different semantics.

\subsection{Benchmarks}
\label{sub:benchmarks}

These benchmarks ignore the time needed to load the data into R, which is on the order of 3-5 seconds once it has been serialised in R's binary dataformat.  Unfortunately due to inefficiencies in the implementation of R's data frame strucutre, data frames can not be used to work with very large vectors without substantial performance penalities. Future work will hopefully address some of the deficiencies in R's data model and make it easier to work with large datasets.


\begin{figure}[htb]
 \centering
%  \includegraphics[width=1.5in]{sample}
 \caption{Sample illustration.}
\end{figure}

\section{Future work}
\label{sec:conclusion}

While the reference implementation is in-memory and single-core, parallelisation and out-of-memory datastores are obvious areas for future work. We expect that fine pre-binning with simple summaries may be done in existing datastore; the grouping algorithm is trivial to implement in SQL, and most databases already provide simple summary statistics like the mean. Other systems, e.g.\ madlibs \citep{hellerstein:2012}, may provide richer summaries making grouping and summarisating in the database even more appealing. However, databases are is likely to be high-throughput and higher-latency than local computing. This suggests a workflow where we supplement fine prebinning in the database with coarser binning locally, especially when supporting interactive graphics.

Something about column stores: \citep{kersten:2011}

Visualisation is only one piece of the data analysis puzzle: we also need tools for transforming and modelling big data. 

%% if specified like this the section will be committed in review mode
\acknowledgments{
The authors wish to thank Yue Hue who coded up many of the initial prototypes. And to JJ Allaire, Dirk Eddelbuettel, Romain Francois and Carlos Scheidegger for their help with C++. Early versions of this work were generously sponsored by Revolution Analytics.}

\bibliographystyle{abbrv}
% bibtool -x bigvis > references.bib
\newpage
\bibliography{references}
\end{document}
