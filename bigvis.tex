\documentclass[journal]{vgtc}                % final (journal style)
%\documentclass[review,journal]{vgtc}         % review (journal style)
%\documentclass[widereview]{vgtc}             % wide-spaced review
%\documentclass[preprint,journal]{vgtc}       % preprint (journal style)
%\documentclass[electronic,journal]{vgtc}     % electronic version, journal

%% Please note that the use of figures other than the optional teaser is not permitted on the first page
%% of the journal version.  Figures should begin on the second page and be
%% in CMYK or Grey scale format, otherwise, colour shifting may occur
%% during the printing process.  Papers submitted with figures other than the optional teaser on the
%% first page will be refused.

\usepackage{mathptmx}
\usepackage{graphicx}
\graphicspath{{./images/}}
\usepackage{times}
\usepackage{natbib}
\usepackage{booktabs}
\usepackage{amsmath}
\usepackage{mathtools}

%% This turns references into clickable hyperlinks.
\usepackage[bookmarks,backref=false,linkcolor=black]{hyperref} %,colorlinks
\hypersetup{
  pdfauthor = {},
  pdftitle = {},
  pdfsubject = {},
  pdfkeywords = {},
  colorlinks = true,
  linkcolor = black,
  citecolor = black,
  pageanchor = true,
  urlcolor = black,
  plainpages = false,
  linktocpage
}

%% If you are submitting a paper to a conference for review with a double
%% blind reviewing process, please replace the value ``0'' below with your
%% OnlineID. Otherwise, you may safely leave it at ``0''.
\onlineid{0}

%% declare the category of your paper, only shown in review mode
\vgtccategory{Research}

%% allow for this line if you want the electronic option to work properly
% \vgtcinsertpkg

%% In preprint mode you may define your own headline.
%\preprinttext{To appear in an IEEE VGTC sponsored conference.}

%% Paper title.

\title{Bin-summarise-smooth: A framework for visualising large data}

%% indicate IEEE Member or Student Member in form indicated below
\author{Hadley Wickham}
\authorfooter{
%% insert punctuation at end of each item
\item
 Hadley Wickham is Chief Scientist at RStudio. E-mail: h.wickham@gmail.com.
}

%other entries to be set up for journal
\shortauthortitle{Wickham: A visualisation framework for in-memory big data}

%% Abstract section.
\abstract{

Visualising large data is challenging both perceptually and computationally: it is hard to know what to display and hard to efficiently display it once you know what you want. This paper outlines a framework to tackle both problems, based around a four step process of bin, summarise, smooth, and visualise. Binning and summarising efficiently (O(n)) condense the large raw data into a form suitable for display (recognising that there are $\sim 3$ million pixels on a screen). Smoothing helps resolve problems from binning and summarising, and because it works on smaller, condensed datasets, it can make of use algorithms that are more statistical efficient even though they're more computationally expensive.

The paper is accompanied by single-core in-memory reference implementation that produces static graphics in R, the {\tt bigvis} package. The framework is readily extensible to parallel out-of-memory solutions, and to interactive and dynamic graphics.
} 

%% Keywords that describe your work. Will show as 'Index Terms' in journal
%% please capitalize first letter and insert punctuation after last keyword
\keywords{Big data, kernel smoothing.}

%% ACM Computing Classification System (CCS). 
%% See <http://www.acm.org/class/1998/> for details.
\CCScatlist{ % not used in journal version
  \CCScat{I.5.1}{Pattern recognition}{Models}{Statistical};
  \CCScat{G.3}{Probability and statistics}{Statistical computing};
}

\teaser{
  \centering
  \includegraphics[width=5.33cm]{teaser-1}%
  \includegraphics[width=5.33cm]{teaser-2}%
  \includegraphics[width=5.33cm]{teaser-3}
  \caption{Average delay (colour, in minutes) as a funtion of distance (x axis, in miles) and speed (y axis, in mph) for 76 million flights. The visualisation needs refinement to be useful: (Left) all data, (center) focussing on the middle 99.5\% of the data, (right) transforming average delay to shrink the tails and focus on values near 0. Flights with higher than average speeds (top-right) have shorter delays (red); more interestingly, a subset of shorter, slower flights (bottom-left) have average delays close to 0 (white).}
  \label{fig:teaser}
}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%% START OF THE PAPER %%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}
\firstsection{Introduction}

\maketitle

As data grows ever larger, it is important that our ability to visualise it grows too. This paper presents a novel framework for visualising large data that is designed to be both computationally and statistically efficient, dealing with both the challenges of what to display for very large datasets and how to display it fast enough for interactive exploration. The insight that underlies this work is simple: the bottleneck when visualising large datasets is the number of pixels on a screen. At most we have around 3,000,000 pixels to use, and 1d summaries need only 3,000 pixels. There is no point displaying more data than pixels, and rathering than relying on the rendering engine to intelligently reduce the data, we develop summaries built on well-known statistical principles. 

The framework includes the most important 1d and 2d statistical graphics: histograms, frequency polygons and kernel density estimates for 1d data; scatterplots, scatterplot smoothers and boxplots. 

It is readily extensible to new visualisations that instead focus on higher moments (like standard deviation, skewness and kurtosis) instead of the mean or median.

The framework attempts to address the tension between a providing flexible system that can that doesn't restrict the analyst to preconcieved visualisations, but equally is constrained to a domain where computation can be performed efficiently. 

The framework involves four steps: binning, summarising, smoothing and visualising. Binning and summarising condenses the large raw data to a summary on the same order of size as pixels on the screen. To be computational efficient, binning and summarising make some statistical sacrifices, but but these can be compensated for with the smoothing step. Smoothing is computationally harder, but smoothing condensed data is fast and looses little statistical strength. 

Section \ref{sec:condense} discusses binning and summarising, focussing on the computational concerns. Computationally, we want linear behaviour and the ability to parallelise, but statistically, we want summaries that are resistant to unusual values. Section~\ref{sec:smooth} discusses smoothing, focussing more on the statistical side, and shows how we can remedy some of the statistical problems with the fast binning and summary steps. Even once we've reduced the data to manageable size, visualisation of large data presents some special challenges. In Section~\ref{sec:visualise}, we discuss generally how to visualise the condensed datasets, and some of the particular problems with very large data, notably problems caused ``unusual'' values.  

The paper is accompanied by an open-source reference implementation in the form of the bigvis R \citep{R} package. This is available from \url{http://github.com/hadley/bigvis} and is described in Section~\ref{sec:bigvis}. While the reference implementation focusses on in-memory, single-core summaries of continuous data with an eye to produce static graphics, these are not limitations of the framework, and while not the focus of the paper, I will also mention issues with out-of-memory data, parallel multi-core processing and interactive visualisations. 

To illustrate the framework I include figures generated from the flight on-time performance data made available by the Bureau of Transporation Statistics\footnote{\url{http://transtats.bts.gov/Fields.asp?Table_ID=236}}. I use flight performance data for all domestic US flights 2000--2011: $\sim$78,000,000 flights in total. The complete dataset has 111 variables, but here we'll explore just four: the flight distance (in miles), elapsed time (in minutes), average speed (in mph) and the arrival delay (in minutes). The data was mildly cleaned: negative times, speeds greater 761 mph (the speed of sound), and distances greater than 2724 miles (the longest flight in the continental US, {\sc sea}--{\sc mia}) were replaced with missing values. This affected $\sim$1.8 million (2.4\%) rows. The data, and code, needed to reproduce this paper and accompanying figures can be found at \url{http://github.com/hadley/bigvis}.

\section{Related work}
\label{sec:related-work}

% Related statistics work
There is an extensive body of statistical research on density estimation (smooth histograms) and smoothing, stretching back to the early 70s. Three good summaries of the work are \cite{scott:1992,bowman:1997,loader:1999a}.    \cite{hardle:1992,wand:1994,fan:1994} focus on the computational side of the smooth estimators, much of which occured in the early 90s as statisticians worked to transition their algorithms from mainframes to PCs.  Surprisingly none recognise that simple kernel smoothing is a convolution, and in high-dimensions rearranging the order of the convolution can change the problem from $O(n^d)$ to $O(nd)$. But these papers only treat special cases of this framework, fail to treat some of the problems associated with visualising large data and are not accompanied by code.

% Infovis and kde
\citep{lampe:2011} uses kernel density estimates in an infovis context, but fails to connect their work to existing statistical work. Provide a very fast implementation on the GPU, which presumably could be made even faster if they read previous work. Other techniques from infovis are related to kernel density estimation. Footprint splatting \citep{becker:1997,yang:2003} is exactly equivalent to kernel density estimation. Similarly, using transparency as a way of dealing with overplotting, as in ..., can be seen as a special type of kernel density estimate where the kernel is a uniform radial point.

% Other papers on large data
There are other techniques to deal scatterplots of large data. Hexbin plots \citep{carr:1987} bin 2d data using hexagons to avoid the perceptual issues related to gridding. Generalised scatterplots \citep{keim:2010} use distortion, but no discussion of computational performance. \citep{heer:2012} discusses the challenges of visualising large data, and proposes interaction as a solution. Emphasises the challenge of data cleaning and manipulation.

\citep{unwin:2006} aggregates much work from statistical graphics.


\section{Condense}
\label{sec:condense}

The first step in the framework is to condense the large original dataset to binned summaries which we will later visualise. Figure~\ref{fig:condense} illustrates the principle with the distance variable. Distance is binned in to 10-mile-wide bins, and each bin is summarised with three counts: the count, the average speed, and the standard deviation of the speed. To do this we first assign each observation to an integer bin, as described in Section~\ref{sub:bin}; and then reduce all observations in a bin to a handful of summary statistics, as described in Section~\ref{sub:summarise}.

\begin{figure}[htb]
 \centering
 \includegraphics[width=\linewidth]{condense}
 \caption{Distance binned into 273 10-mile-wide bins, summarised with count, mean speed and standard deviation of speed. Note the varying scales on the y-axis.}
 \label{fig:condense}
\end{figure}

\subsection{Bin}
\label{sub:bin}

We first assign each observation to an integer representing a bin. We want an injective mapping, so that many data points will be mapped to a single integer.
We use {\bf fixed width binning}. Fixed width binning is extremely computationally efficient, easily extended from 1d to nd, and while statistically na\"ive, there is little evidence that variable binwidths do better. 

It is important to think of binning as a separate step, as it may be performed outside of the visualisation environment, for example in the database layer (integers require half as much space as doubles, so this could half data transmission costs). 

\subsubsection{Computation}

Fixed width binning is parameterised by two variables, the origin (the position of the first bin) and the width. The computation is shown in \eqref{eq:fixed-width}.

\begin{equation}
  \label{eq:fixed-width}
  \left \lfloor \frac{x - \text{origin}}{\text{width}} \right \rfloor + 1
\end{equation}

Bins are 1-indexed, reserving bin 0 for missing values and values smaller than the origin.

\subsubsection{Extension to nd}

Fixed width binning can be easily extended to multiple dimensions. You first bin each dimension, producing a vector of integers $(x_1, x_2, ..., x_m)$. It is then straightforward to devise a bijective map between this vector and a single integer, given that we know the largest bin in each dimension. If we have $m$ dimensions, each taking possible values $0, 1, \ldots, n_m$, then we can collapse the vector of integers into a single integer using this formula:

\begin{equation}
  \label{eq:nd-bins}
  \begin{matrix*}[l]
   = & x_1 + x_2 \cdot n_1 + x_3 \cdot n_1 \cdot n_2 + \cdots + x_m * \Pi^{n-1}_{i = 1} n_i \\
   = & x_1 + n_1 \cdot (x_2 + n_2 \cdot (x_3 + \cdots(x_m))
  \end{matrix*}
\end{equation}

It is easy to see how this works if each dimension has ten bins. For example, to project 3d bin $(5, 0, 4)$ into 1d, we compute $5 + 0 * 10 + 4 * 100 = 4 + 10 * (0 + 10 * 5) = 405$. Given a single integer, we can find the vector of $m$ original bins by reversing the transformation, peeling off the integer remainder after dividing by 10. For example, 1d bin $356$ corresponds to 3d bin $(6, 5, 3)$.

This function is a monotone minimal perfect hash, and highly efficient hashmap variants are available that make use of its special properties \citep{belazzougui:2009}. For example, because the hash is perfect, we can eliminate the equivality comparison that is usually needed after a candidate has been found with the hashed value. Even if this data structure is not used (as in the reference implementation), it easy to efficiently summarise bins in high-dimensions using standard data structures: either a vector if most bins have data in them, or a hashmap if not.

The challenges of working with nd summaries are typically perceptual, rather than computational. Figure~\ref{fig:condense-2d} shows a 2d summary of distance and speed: even moving from 1d to 2d makes it significantly harder to accurately percieve count \citep{cleveland:1984}.

\begin{figure}[htb]
 \centering
 \includegraphics[width=\linewidth]{condense-2d}
 \caption{Distance and speed summarised with count. Note that count scale has been transformed slightly, as described in Section~\ref{sub:mt}, to draw attention to regions with fewer counts.}
 \label{fig:condense-2d}
\end{figure}

\subsubsection{Statistical limitations}

Ideally, we would like bigger bins in regions with few data points because the error of the summary is typically $\Theta(1 / \sqrt{n})$. For example, in Figure~\ref{fig:condense} we would like to have bigger bins for larger distances since the counts are small and the estimates of mean and standard deviation are more variable than we would like. However, there is little evidence that varying binwidths leads to asymptotically lower error \citep{terrell:1992}; while varying bin widths should provide a better approximation to the underlying data, the optimisation problem is so much more challenging that any potential improvements are lost. Instead of resolving these issues with a more sophisticated binning algorithm, we will fix them in the later smoothing step.  

% Linear binning vs. simple binning

% proviso that the bin is a integer, so can not store more than $2^32$ possible bins. (This seems like a lot but shrinks quickly as the number of dimensions grows: it's 85 bin in each direction in 5d and only 9 bins in each direction in 10d).  Outside that range can switch to standard hashing algorithms, albeit with a performance penalty because the algorithm will be more complex, and additional equality comparisons will be required. (Read citations on \url{http://en.wikipedia.org/wiki/Perfect_hash_function} and cite appropritately)

\subsection{Summarise}
\label{sub:summarise}

Once each of the $n$ original data points has been placed into one of $m$ integer bins ($m \ll n$), the next step is to collapse the points in each bin into a small number of summary statistics. Picking useful summary statistics requires balancing between computational efficiency and statistical robustness. 

\subsubsection{Computationl efficiency}

Gray et al. \citep{gray:1997} provide a useful classification for understanding the computational characteristics of a summary statistic. A summary is:

\begin{itemize}
  \item {\bf distributive} if it can be computed using a single element of interim storage and summaries from subgroups can be combined. This includes count, sum, min, and max.
  
  \item {\bf algebraic} if it is a combination of a fixed number of distributive statistics. This includes the mean (count + sum), standard deviation (count + sum + sum of squares) and higher moments like skewness and kurtosis.
  
  \item {\bf holistic} if it requires interim storage that grows with the input data. This includes quantiles (like the median), count of distinct elements or the most common value. 

\end{itemize}

Algebraic and distributive statistics are important because results from subgroups can easily be combined. This has two benefits: it makes parallelisation trivial, and it supports a tiered approach to exploration. For example, if you have 100 million observations, you might first finely bin into 100,000 bins. Then for any specific 1d plot, you rebin or subset the fine bins rather than the original data. This tiered approach is particularly useful for interactive visualisations; the fine binning can be done upfront when the visualisation is created, then modifying the binwidth or plot limits can be done interactively.

It is often possible to convert a summary from holistic to algebraic by taking an approximation. For example, the count of distinct values can be approximated with the hyperloglog algorithm \citep{flajolet:2007}, the median with the remedian \citep{rousseeuw:1990}, and other quantiles with other methods \citep{finkelstein:1994,hurley:1995,liechty:2003}. Others have proposed general methods for approximating any holistic summary \citep{christmann:2007}.

The mean, standard deviation and higher moments can all be computed in a single pass, taking $O(n)$ time and $O(1)$ memory. Some care is needed as naive implementations (e.g.\ computing the variance as $\sum_i^n x_i^2 / n - \left( \sum_i^n x_i / n \right)^2$) can suffer from severe numerical problems, but better algorithms are well known \citep{welford:1962}. The median also takes $O(n)$ time (using the quick-select algorithm), but needs $O(n)$ memory: there is no way to compute the median without storing at least half of the data, and given the median of two subgroups, no way to compute the median of the full dataset.

\subsubsection{Statistical robustness}

There is an interesting tension between the mean and the median: the median is much more robust to unusual values than the mean, but requires unbounded memory. A useful way to look at the robustness of a statistic is the {\bf breakdown point}. The breakdown point of a summary is the proportion of observations an attacker needs to control before they can arbitrarily influence the resulting summary. It is 0 for the mean: if you can influence one value, you can force the mean to be any value you like. The breakdown point for the median is 0.5: you have to taint 50\% of the observations before you can arbitrarily change the median. The mean is computationally desirable, but is less statistically desirable since just one flawed value can arbitrarily taint the summary. This is a general problem: the easiest summary statistics to compute are also the least robust, while robust statistics are usually holistic.

Even if you do use robust statistics, you are only protected from scattered outliers, not a radical mismatch between the data and the summary. For example, a single measure of central tendency (mean or median) will never be a good summary if the data has multiple modes. Compare Figures~\ref{fig:condense} and \ref{fig:condense-2d}: for shorter flights, there appears to be multiple modes of speed for a given distance and so the mean is not a good summary. Visualisation must be iterative: you can not collapse a variable to a single number until you have some confidence that the summary isn't throwing away important information. In practice, users may need to develop their own summary statistics for the pecularities of their data; the ones discussed here should provide a good start for general use.

\subsubsection{Higher dimensions}

There is no reason to limit ourselves to only 1d summary functions. 2d summaries like the correlation may also be interesting. All statistics from a linear model can be computed in $O(n)$ in time and $O(1)$ in space \citep{miller:1992}, and are and thus suggest a fruitful ground for generalisations to higher dimensions. Other quickly computed 2d summaries are also of interest; scagnostics \citep{wilkinson:2005} are an place to start.

\section{Smooth}
\label{sec:smooth}

Smoothing is an important step because it allows us to resolve problems with excessive variability in the summaries. This variability may arise because the bin size is too small, or because there are unusual values in the bin. Figure~\ref{fig:smooth} shows the results of smoothing Figure~\ref{fig:condense}: much of the small-scale variation has been smoothed away, making it easier to focus on the broad trends. There is some suggestion that the standard deviation of speed is lowest for distances of 1000--1500 miles, and rises for both smaller and large distances. This is much harder to see in Figure~\ref{fig:condense}.

\begin{figure}[htb]
 \centering
 \includegraphics[width=\linewidth]{smooth}
 \caption{The same underlying data from Figure~\ref{fig:condense}, but smoothed with a bandwidth of 50. This removes much of the uninteresting variation while preserving the main trends.}
 \label{fig:smooth}
\end{figure}

There are many approaches to smoothing, but we use a family of kernel based methods, because they:

\begin{itemize}
  \item are simple and efficient to compute,
  
  \item have a single parameter, the {\bf bandwidth} that controls the amount of smoothing,

  \item work equally well when applied to binned data \cite{wand:1994},
  
  \item are approximately equivalent to other more complicated types of smoothing \citep{silverman:1984}, 
  
  \item form the heart of many existing statistical visualisations such as the kernel density plot \citep{scott:1992}, average shifted histogram \citep{scott:1985} and loess \citep{cleveland:1979}.

\end{itemize}

\subsection{How it works}

Figure~\ref{fig:smooth-types} illustrates the progression of smoothing methods from fast and crude to sophisticated and slow. The simplest smoothing method (top) is the binned mean, where we divide the data in bins and compute the mean of each bin. This is simple, but is locally constant and not very smooth. The next step up in complexity is the running (or nearest neighbour) mean where we average the five nearest points at each location. This is a considerable improvement, but is still rather jagged.

\begin{figure}[htb]
 \centering
 \includegraphics[width=0.75\linewidth]{smooth-types}
 \caption{Five types of smoothing on a artificial dataset generated with $\sin(x)$ on $[0, \pi]$, with random normal noise with $\sigma = 0.2$, and an outlier at $\pi / 2$. Smoothes are arranged from simplest (top, binned) to most accurate (bottom, robust local regression). To aid comparison each smooth is shown twice, prominently with a thick black line, and then repeated on the next plot in red to make comparison easier. The subtlest difference is the between the kernel mean and kernel regression: look closely at the boundaries.}
 \label{fig:smooth-types}
\end{figure}

The remaining three types use a simple idea: we want to not only use the neighbouring points, but we want to weight them according to their distance from the target point. In statistics, the weighting function is traditionally called a {\bf kernel}. There are a huge variety of kernels, but there is little evidence to suggest that the precise form of the kernel is important \citep{cleveland:1996}. Gaussian kernels are common, but I use the triweight, $K(x) = (1 - |x|^3)^2 I_{|x| < 1}$, because it is bounded and simple (evaluation of this function is $\sim 10 \times$ faster than the Gaussian).  

At each bin $i$ we have $x_i$, the center of the bin, $y_i$, the summary statistic, and $w_i$, the number of observations in the bin. To predict a smooth estimate at position $j$, we first compute the kernel weights for each location $k_i = K(\frac{x_j - x_i}{h})$. The parameter $h$ is called the bandwidth, and controls the degree of smoothing: larger $h$'s include more neighbours and produce a smoother final curve. Because of the form of the triweight kernel, any observation more than $h$ away from $x_j$ will not contribute to the smoothed value, thus enabling efficient computation.  Once we have these weights, generating the smooth is a simple matter of using them with an existing statistical technique.

The three kernel techniques, kernel means (aka Nadaraya-Watston smoothing), kernel regression (aka local regression) and robust kernel regression (aka loess), make a tradeoff between performance and quality. While closely related, these methods developed in different parts of statistics at different times, and the terminology is often inconsistent. I have attempted to provide a unifying description here; \citep{cleveland:1996} provides a good historical overview of their development.

The kernel (weighted) mean is fastest to compute but suffers from bias on the boundaries, because the neighbours only lie on one side. The kernel (weighted) regression overcomes this problem by effectively using a first-degree Taylor approximation. In Figure~\ref{fig:smooth-types}, you can see that the kernel mean and kernel regression are coincident everywhere except near the boundaries. Higher-order approximations can be used by fitting higher order polynomials in the model, but there seems to be little additional benefit in practice \citep{cleveland:1996}.

Finally, the robust kernel regression iteratively down-weights the effect of points far away from the curve, and reduces the impact of unusual points at the cost of increased computation time. There are many ways to implement robust regression, but the procedure used by loess \citep{cleveland:1979} is simple, computational tractable and performs well in practice. The advantage of a robust smooth can be seen in Figure~\ref{fig:smooth-types}: it is the only smoother unaffected by the unusually low point at $\pi / 2$.

\subsection{High-dimensions and performance characteristics}

The three kernel techniques are readily extended to higher dimensions, using corresponding higher dimensional means and regressions. Kernel means are particularly easy to efficiently extend to higher dimensions  because they are a convolution, and due to the associative nature of convolution it's possible to perform a $d$-dimensional smooth with a sequence of $d$ 1d smooths. This is an important optimisation because it converts an $O(n^d)$ process to $O(nd)$. You can perform kernel regression in a similar way, although it is only exact if the grid of values is uncorrelated. Robust kernel regression can not be approximated in this way, and must be performed exactly.

Given that the kernel mean is a convolution, it's natural to wonder if a discrete fast fourier transform (dFFT) would be useful. My experience is that it is not helpful in this case: it requires more complicated code to deal with the periodic nature of the dFFT; it is less flexible if you want to predict smooth values at only a subset of locations; and it provides only neglible performance improvements in 1d \citep{wand:1994}.

% \subsection{Kernel density estimates vs.\ kernel smoothed counts}
% 
% Note that when applied to count summaries, these methods are subtly different from a kernel density estimate. A kernel mean is similar to a kernel density estimate: but a kernel mean divides by (normalises by) by the total weight at each point. This change is most noticeable with small bandwidths: the kernel density estimates will be very large (because they must integrate to 1), and outside the range of the data the estimates will be 0, where outside the range of the data the smoothed counts will be undefined (0/0).

\subsection{Automatic bandwidth selection}

Kernel smoothing introduces a tuning parameter which controls the degree of smoothing: the bandwidth. There is much statistical research on how to pick the ``best'' bandwidth but is typically framed in the content of reducing integrated mean squared error, and it is not obvious that this corresponds with what we want for visualisation \citep{denby:2009}. Following \citep{loader:1999}, I take a pragmatic attitude and use leave-one-out cross-validation ({\sc loocv}) \citep{efron:1983} to provide a starting point for the user. In a realistic data analysis scenario, this would be supplemented with interactive controls so the user could explore findings at different levels of resolution.

The intution behind {\sc loocv} is simple: we compare the actual statistic at each location with its smoothed prediction computed without that observation. The observation level errors are summarised with the root mean squared error (rmse): $\sqrt{ \sum (y_i - \hat{y}_i)^2 / n}$, and we look for the bandwidth that has the smallest rmse. We can visually explore the rmse across a range of possible grid values, or use standard numerical optimisation (L-BFGS-B \citep{byrd:1995}) to find a minima. Figure~\ref{fig:smooth-rmse} shows the {\sc loocv} rmse error for the three variables in Figure~\ref{fig:condense}. It demonstrates the challenges of relying on numerical optimsation alone: mean and sd have a number of local minima, while count is very flat.

With the bounded tricube kernel, {\sc loocv} is straightforward to implement efficiently in $O(m b)$ where $b$ is the number of bins covered by the kernel ($b \ll m$). \citep{fan:1994} suggests incremental algorithms that may reduce computation time still further.

\begin{figure}[htb]
 \centering
 \includegraphics[width=\linewidth]{smooth-rmse}
 \caption{A leave one out cross-validated estimate of the root-mean-squared-error associated with smoothing count, mean, and sd with varying bandwidths. Error has been capped at 2 to focus on the regions with lower errors.}
 \label{fig:smooth-rmse}
\end{figure}


\subsection{Varying bandwidths}

Intuitively, it seems like we should be able to do better with an adaptive binwidth: using smaller bins where there is more data, or where the curve is especially wiggly. There are many approaches documented in the literature  \citep{terrell:1992, brockmann:1993,schucany:1995,herrmann:1997}. We follow the simple, but performant, approach outlined in \citep{fan:1995}: to find a per-location $h$, we divide the data into $m / (10 * \log(m))$ pieces, estimate the optimal bandwidth for each piece separately, then smooth the estimates.

Figure~\ref{fig:varying-h} illustrates the process for a challenging function proposed by \citep{fan:1995}. It demonstrates the effectiveness of this approach: a fixed bandwidth will either oversmooth on the left or undersmooth on the right, whereas a variable bandwidth can adapt to provide visually equivalent smoothness across the entire range of the function.

\begin{figure*}[htb]
 \centering
 \includegraphics[width=0.25\linewidth]{variable-h-raw}%
 \includegraphics[width=0.25\linewidth]{variable-h-smoothed-fixed}%
 \includegraphics[width=0.25\linewidth]{variable-h-h-est}%
 \includegraphics[width=0.25\linewidth]{variable-h-smoothed-var}%
 \caption{(Far left) A challenging function to smooth with fixed bandwidth \citep{fan:1995}. (Left) Data smoothed using single best bandwidth. (Right) Optimal bandwidth estimated on 10 separate pieces. (Far right) Data smoothed using individual bandwidth at each location.}
 \label{fig:varying-h}
\end{figure*}

\section{Visualise}
\label{sec:visualise}

Once we have binned, summarised and smoothed the raw data, it is relatively straightforward to visualise it. When discussing visualisations, it's useful to distinguish between the variables in the original data set that we bin, and the new variables that we create through our summaries. To do so, we'll adopt the convention that a $(n, m)$-d dataset or visualisation represents a dataset with $n$ binned variables and $m$ summary variables. For example, Figure~\ref{fig:condense} shows 3 $(1, 3)$-d plots (distance + count, mean and sd), Figure~\ref{fig:condense-2d} show a $(2, 1)$-d plot (distance and speed + count), and Figure~\ref{fig:teaser} is another a $(2, 1)$-d visualisation, but produced from a $(2, 2)$-d dataset (distance and speed + mean).

Generally, we're interested in plots where both $m$ and $n$ are one or greater. A $(0, 1)$ plot would just display a single set of summary statistics, and a $(m, 0)$ plot would just show where the bins occurred. A $(0, 2)$ plot might be of interest, because it you could compare summary statistics within each bin (e.g.\ you might be interseted in bins with high mean and low standard deviation), but because it doesn't show the original data, we won't consider it in more detail here. 

We'll focus first on low-d plots, $(1, 1)$-d and $(2, 1)$-d. $(1, 1)$-d plots can be display with a line, and described in Section~\ref{sub:1d-plots}. $(2, 1)$-d plots require a display like Figure~\ref{fig:teaser}, which is often called a heatmap \citep{wilkinson:2009} (although that name is usually reserved for plots of categorical data), or a a tile plot. Section~\ref{sub:2d-plots} discusses these in more detail. To deal with n-d data, Section~\ref{sub:nd-plots} discusses general combinatoric methods for combining low-d plots.

As the size of data grows, the probability of finding very unusual values also grows. In practice, we have found the visualisation of most large datasets look like Figure~\ref{fig:teaser}, with most data concentrated in a small area. We have two tools to deal with this: peeling, when the outliers are spatial, described in Section~\ref{sub:peeling}; and the modulus transformation, when the outliers are in colour, size or another aesthetic, described in Section~\ref{sub:mt}.

Finally, it's also important to preserve missing values: this is not an issue just for big data, but it's worth mentionining here. Section~\ref{sub:missing} discusses the importance of missing data.

\subsection{\texorpdfstring{$(1,1)$}{(1, 1)}-d plots}
\label{sub:1d-plots}

f the summary statistic is a count, this produces a frequency polygon \citep{scott:1985a} which is an improvement on the traditional histogram as it is easier to overlay multiple frequency polygons in one plot, and it unifies the display of counts and other summaries. If the statistic is not a count, we also need some way of displaying the count: this is important because we don't draw strong conclusions about areas with weak data support.

\subsection{\texorpdfstring{$(2,1)$}{(2, 1)}-d plots}
\label{sub:2d-plots}

Techniques from cartography, e.g.\ \citep{kennelly:2002}, be used to improve the perception of such surfaces.

\subsection{\texorpdfstring{$(n, m)$}{(n, m)}-d plots}
\label{sub:nd-plots}

Extending these graphics to higher dimensions is challenging, but two general avenues of attach exist: small multiples and interaction.

Small multiples (aka facetting) to display low-d slices through a high-d dataset. The product plots \citep{me:prodplots} framework explores this in detail for count summaries of categorical data, and is straightforward to extend to include count summaries of binned continuous data because once continuous data has been binned, it can be treated the same way as discrete data. It requires a small additional constraint that high-levels need to be relatively coarsely binned, but otherwise applies in a straightforward manner.

An alternative approach is to use interaction. For example, connectd multiple low-dimensional displays through linked brushing \citep{liu:2013,swayne:2003}.  Interacting through low-d plots can also reduce the data summary burden.

\subsubsection{Peeling}
\label{sub:peeling}

A simple approach to deal with spatial outliers is to remove the smallest outer values. I call this approach peeling, and implemented it by progressively removing the smallest counts on the convex hull of the data. Figure~\ref{fig:teaser} illustrates the utility of this technique: even peeling off a tiny fraction of the data (0.5\%) yields a substantially improved plot. Some small amount of peeling (often $< 1\%$) seems to improve most 2d plots, drawing the focus to the bulk of the data. For plots of non-count summaries, this also has the additional benefit of reducing outliers in the summary dimension. 

Typically, you don't want to ignore these unusual values, but instead you partition your analysis to look at the common and at the unusual.

\subsubsection{Modulus transformation}
\label{sub:mt}

Removing bins with small counts will sometime remove the most outlying values in the summary dimension, but often many outliers will remain. It's useful to have a flexible technique to downweight the visual impact of these unusual values, without removing them altogether.  I suggest using the modulus transformation which provides a flexible tool to shrink or expand the tails of a distribution. Figure~\ref{fig:teaser} shows how a mild transformation 

The modulus transformation \citep{john:1980} is a generalisation of the box-cox transformation \citep{box:1964} to deal with positive and negative data. The $\lambda$ parameter controls the strength of the transformation. The strongest transformation is at $\lambda = 0$, which performs a log-transformation. 

\begin{equation}
\begin{cases} 
  \text{sgn}(x) \cdot \log(|x| + 1) & \text{if $\lambda = 0$,} \\
  \text{sgn}(x) \cdot \frac{(|x| + 1)^\lambda - 1}{\lambda} &\text{if $\lambda \ne 0$.}
\end{cases}
\end{equation}

The modulus transformation is particularly useful in conjunction with interactivity, as it allows the user to dynamically focus on important parts of the distribution.

\subsection{Missing values}
\label{sub:missing}

A final design principle is that missing values should never be omitted without user intervention: they should always be preserved along the entire route from raw data to final visualisation. It is possible to remove them, but it must be a deliberate decisions: you never want to inadvertently distort your visualisations by silently leaving off data. This is inspired by {\sc manet} \citep{unwin:1996} and is particularly important for real data, as as in the flights data, many suspect values may have been replaced with missing values. This principle is generally simple to implement, but must be carried through all other components of the framework: missing values must be preserved during binning, summarising and smoothing.

That said, in the intersts of space none of the plots in this paper actually show missing values. Figure~\ref{fig:missing} shows why this is a bad idea: the distribution of speed is very different depending on whether or not distance is missing. Distance is clearly not missing at random, so ignoring it may lead to skewed conclusions.

\begin{figure}
  \centering
   \includegraphics[width=\linewidth]{speed-distance}
 \caption{The distribution of speed depending on whether distance is present or missing.}
 \label{fig:missing}
\end{figure}

\section{The {\tt bigvis} package}
\label{sec:bigvis}

The bigvis package provides a reference implementation of the bin-summarise-smooth framework with an open-source R package available from \url{http://github.com/hadley/bigvis}. It has two main parts: a high-performance implementation of the key algorithms written in C++ and a user-friendly wrapper to supports exploratory data analysis written in R. The key to bridging the two pieces is the Rcpp package \citep{eddelbuettel:2011} which provides an easy way to access R's internal datastructures with a clean C++ API. 

The C++ component makes use of templated functions to avoid the cost of virtual method lookup (this is small, but adds up with potentially 100's of millions of invocations). This adds an additional challenge when connecting R and C++ because templated functions are specialised at compile time, but the R code needs to call them dynamically at run-time. To work around this problem, a small code generator produces all specialised versions of templated functions. This is somewhat inelegant, but unavoidable when coupling programming languages with such different semantics.

The bigvis {\sc api} is designed to make visualising large datasets familiar to R users. It implements methods for the {\tt autoplot} generic providing default visualisations designed with the principles above, and also cleanly interfaces with other plotting tools in R, like ggplot2 \citep{me:ggplot2,wickham:2007d}. This makes it easy to build custom graphics based on the condensed and smoothed data if the default graphics are not sufficient. The figures in this paper were all drawn with ggplot2, and are customised to make the most of limited space.

The reference implementation supplies many of the constraints that have guided this paper. Bigvis is designed to be used by experienced analysts in conjunction with other data manipulation tools and statistical models. My goal was for users to be able to plot $10^8$ observations in under 5s on commodity hardware. $10^8$ doubles occupy a little less than 800 Mb, so about 20 vectors can be stored in 16 Gb of ram, with about 1 Gb left over. Five seconds is well above the threshold for direct manipulation, but is in line with how long other data manipulationg and modelling functions take in R. Too much longer than 5s and it becomes tempting to give up waiting and switch to another case, which breaks flow and reduces productivity. To calibrate, 5s is about how long it takes to draw a regular scatterplot of 200,000 points in R, so bigvis is several orders of magnitude faster.

\subsection{Benchmarks}
\label{sub:benchmarks}

Figure~\ref{fig:benchmark} provides basic benchmarks for the bigvis package, over a range of data input sizes, bins and summary statistics. Timings were performed on a 2.6 GHz Intel Core i7, 15\" MacBook retina with 16 Gb ram and a solid state harddrive. The benchmarks assume the data are already in memory: loading data from R's binary data format takes around 2s for $10^8$ observations.

Computing time grows linearly with input size and slowly with number of bins (only conditions where there were 10 or more observations in each bin shown). There is relatively little variation between the count, standard deviation and mean. The three summaries are all $O(n)$, but the standard deviation has a higher constant than the mean, and the median is slowest because it requires a complete copy of the data. 

These timings are lower bounds on potential performance: I have been programming in C++ for less than six months so the C++ code is by no means expert and uses only a smattering of advanced C++ functions. There are likely to be considerable opportunities for further optimisation.

\begin{figure}
  \centering
   \includegraphics[width=\linewidth]{benchmark}
 \caption{Benchmarking bigvis package varying the input size (colour), number of bins (x-axis) and summary statistic (shape and line type). Note that both x and y axes are logged.}
 \label{fig:benchmark}
\end{figure}

\section{Future work}
\label{sec:conclusion}

The bin-summarise-smooth framework provides a flexible toolkit for the visualisation of large datasets. 

While the reference implementation is in-memory and single-core, parallelisation and out-of-memory datastores are obvious areas for future work. We expect that fine pre-binning with simple summaries may be done in existing datastore; the grouping algorithm is trivial to implement in SQL, and most databases already provide simple summary statistics like the mean. Other systems, e.g.\ madlibs \citep{hellerstein:2012}, may provide richer summaries making grouping and summarisating in the database even more appealing. Column store databases \citep{kersten:2011} provide an even more natural fit with statistical data, and maybe provide efficent out-of-memory backends. Computation on the GPU is appealing, especially outlined in \citep{liu:2013}

This paper has focussed on continuous data, but it is straightforward to extend to categorical variables with relatively few possible values. They can trivially be mapped to integers, and smoothing is typically not appropriate. The challenge arises with the visualisation, some seriation \citep{hahsler:2008} is often needed to draw out important patterns.  An additional challenge is when the number of categories grows with the size of the data: if there are more categories than pixels, figuring out sensible bins is hard.

Finally, it's important to note that visualisation is only one piece of the data analysis puzzle: we also need tools for transforming and modelling big data. Visualisation is excellent for raising and refining questions, but does not scale well: a human needs to look at each plot. Statistical models and other algorithms scale well, but never reveal what you fundamentally don't expect. Tools for visualisation must interconnect with tools for modelling, so practitioners can take advantages of the strengths of both toolsets.

%% if specified like this the section will be committed in review mode
\acknowledgments{
I'd like to thank Yue Hu who coded up the initial prototypes, as well as JJ Allaire, Dirk Eddelbuettel, Romain Francois and Carlos Scheidegger who helped me learn C++ and answered many of my dumb questions. Early versions of this work were generously sponsored by Revolution Analytics and Google.}

\bibliographystyle{abbrv}
% bibtool -x bigvis > references.bib

\bibliography{references}
\end{document}
